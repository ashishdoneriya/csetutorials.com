<!doctype html><html lang=hi><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload href=https://fonts.gstatic.com/s/roboto/v47/KFOkCnqEu92Fr1Mu51xFIzIXKMnyrYk.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=https://fonts.gstatic.com/s/roboto/v47/KFOkCnqEu92Fr1Mu51xMIzIXKMnyrYk.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=https://fonts.gstatic.com/s/roboto/v47/KFOkCnqEu92Fr1Mu51xEIzIXKMnyrYk.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=https://fonts.gstatic.com/s/roboto/v47/KFOkCnqEu92Fr1Mu51xLIzIXKMnyrYk.woff2 as=font type=font/woff2 crossorigin=anonymous><link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100..700;1,100..700&family=Roboto:ital,wdth,wght@0,75..100,100..900;1,75..100,100..900&display=swap" rel=stylesheet><link rel=stylesheet href=https://csetutorials.com/scss/main.min.css><title>Web Crawler Design Techniques</title>
<meta name=description content="Key Points regarding web crawler designing techniques."><meta name=author content="Ashish Doneriya"><meta name=robots content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"><link rel=canonical href=https://csetutorials.com/web-crawler-design-techniques.html><meta property="og:type" content="article"><meta property="og:title" content="Web Crawler Design Techniques"><meta property="og:description" content="Key Points regarding web crawler designing techniques."><meta property="og:url" content="https://csetutorials.com/web-crawler-design-techniques.html"><meta property="og:site_name" content="Cse Tutorials"><meta property="article:published_time" content="2021-04-03T10:37:44+05:30"><meta property="article:modified_time" content="2021-04-03T10:37:44+05:30"><meta property="article:author" content="Ashish Doneriya"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Web Crawler Design Techniques"><meta name=twitter:description content="Key Points regarding web crawler designing techniques."><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"Web Crawler Design Techniques","description":"Key Points regarding web crawler designing techniques.","author":{"@type":"Person","name":"Ashish Doneriya"},"keywords":[],"articleSection":["\"%!s(MISSING)\""],"datePublished":"2021-04-03T10:37:44\u002b05:30","dateModified":"2021-04-03T10:37:44\u002b05:30","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/csetutorials.com\/web-crawler-design-techniques.html"},"url":"https:\/\/csetutorials.com\/web-crawler-design-techniques.html"}</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-VQ66771MZZ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-VQ66771MZZ")</script></head><body><nav class="navbar navbar-expand-lg navbar-dark bg-dark"><div class=container><a class=navbar-brand href=/>Cse Tutorials</a>
<button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse justify-content-end" id=navbarNav><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/about-us/>About us</a></li><li class=nav-item><a class=nav-link href=/contact-us/>Contact us</a></li><li class=nav-item><a class=nav-link href=/privacy-policy/>Privacy Policy</a></li><li class=nav-item><a class=nav-link href=/terms-of-service/>Terms of Service</a></li><li class=nav-item><a class=nav-link href=/topics/>Topics</a></li></ul></div></div></nav><main><article class=page-wrapper><h1 class="mb-4 text-center">Web Crawler Design Techniques</h1><div class="d-flex justify-content-center align-items-center mb-4 text-muted small"><time datetime="2021-04-03 10:37:44 +0530 +0530">Apr 3, 2021</time>
<span class=mx-2>|</span>
<span>Ashish Doneriya</span>
<span class=mx-2>|</span>
<a class=text-decoration-none href=/topics/system-design rel=category>System Design</a></div><div class=post-content><p>A web crawler can be used in various services such as</p><ul><li>Search Engines</li><li>Archive Storage</li><li>Piracy Detector</li><li>Web Mining</li></ul><p>Lets talk about the data flow first. Lets say we have a set of url as a starting point. So first we fetch the content of those urls. After that we parse the content, check if it is duplicate or not. Then we extract urls and after filtering the urls we add those urls to the queue to again crawling. This is the three line summary.</p><p>Lets break this whole process into different tasks and assign them to different components. So the various components used are</p><ol><li>Seed URLs</li><li>URL Frontier</li><li>HTML Downloader & DNS Resolver</li><li>Content Parser</li><li>Duplicate Content Checker</li><li>Link Extractor</li><li>URL Filter</li><li>URL Seen?</li><li>URL Storage</li></ol><p>Let me describe their functions one by one</p><h2 id=components>Components</h2><h3 id=seed-urls>Seed URLs</h3><p>Seed URLs act as starting point. A good seed url can provide as many links as possible.</p><h3 id=url-frontier>URL Frontier</h3><ul><li>URL frontier is a component which acts a storage for URLs to be downloaded. You can assume this as a FIFO queue.</li><li>The url frontier would store the data in both disk and memory because memory space is limited and disk speed is limited. We would use buffers.</li></ul><h3 id=html-downloader--dns-resolver>HTML Downloader & DNS Resolver</h3><p>HTML downloader downloads webpages from the internet whose URLs provided by the URL Frontier. The HTML downloader calls the DNS Resolver to get the IP address of the particular URL.</p><ul><li>The DNS Resolver is very slow and is single threaded. Therefore we require a separate component which is responsible for handling dns queries.</li><li>The crawler cannot send bulk queries to a website to download its content because it may cause a ddos attack and that website could blacklist the web crawler.</li><li>Don&rsquo;t forget to read robots.txt first. Can be cache for better speed.</li><li>Maintain timeout.</li></ul><h3 id=content-parser>Content Parser</h3><p>After downloading the web page, this component parses the webpage and also check if it is a valid page or malformed page.</p><h3 id=duplicate-content-checker>Duplicate Content Checker</h3><p>This component checks whether the content is duplicate of already save pages. One way to check the duplicate content is compare the character of each page one by one with all the pages available in the crawler. The another is to check the hash code of the content.</p><h3 id=link-extractor>Link Extractor</h3><p>This component extracts links from the web pages. Relative urls will be converted to absolute urls.</p><h3 id=url-filter>URL Filter</h3><ul><li>This component filters url.</li><li>The crawler can filter urls based on their lengths (because of spider traps).</li><li>We can also filter blacklisted urls or different types of extentions.</li></ul><h3 id=url-seen>URL Seen</h3><p>This component checks whether the extracted url is already processed or not. If it is not processed then the crawler will add the url to the URL Frontier. We can also take help of bloom filter here.</p><h2 id=things-to-consider>Things to consider</h2><h3 id=dfs-vs-bfs-approach>DFS vs BFS Approach</h3><p>Usually BFS approach is used in webcrawlers because suppose if there is a site like wikipedia then it will take very long time to crawl that website.</p><h3 id=role-of-url-frontier-in-politeness-priority--freshness>Role of URL Frontier in Politeness, Priority & Freshness</h3><h4 id=politeness>Politeness</h4><p>Previously I mentioned that a crawler should avoid sending too many requests to the same site within a short interval. Therefore it is better for the crawler to send request one by one and should add a delay between requests.</p><p>To achieve this politeness, divide the fifo queue of URL Frontier into <strong>multiple FIFO queues</strong>. Each queue would belong to urls have same host. After that there is <strong>Queue Selector</strong> which takes urls from those queues and pass to the <strong>Worker Threads</strong> and those worker threads downloads web pages one by one from the same host.</p><h4 id=priority>Priority</h4><p>We have to maintain the priority of the pages to download eg. a forum question or an index page etc. We have to assign different priorities to different urls in the same website.</p><p>To manage URL priority first every url will be passed to prioritizer who will assign priority to each url and according to its priority, add that url to a queue. There would be different queues for different priorities. After that there is a queue selector who will select urls from queues based on their priorities.
<strong>NOTE :</strong> Priority system comes before politeness.</p><h3 id=distributed-crawling>Distributed Crawling</h3><p>Crawl jobs can be distributed into multiple servers for high performance. You can also distribute crawlers geographically for faster download time.</p><h3 id=server-side-rendering>Server side rendering</h3><p>Now a days a lot of websites use js frameworks and libraries to generate content dynamically. Therefore the crawler could use server side rendering.</p></div></article></main><script async src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js defer></script></body></html>